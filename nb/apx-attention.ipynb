{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAML Attention, Transformers and LLMs (and all the fancy buzzwords)\n",
    "\n",
    "Generative AI, from massive models to agents.\n",
    "These are models that have been trained on such plethora of data\n",
    "that they can solve distinct problems without re-training.\n",
    "One can argue that solveing language understanding\n",
    "the models have indirectly learned solutions to many\n",
    "problems by just leveraging the language structure in the training data.\n",
    "\n",
    "That is all fine and good, traning on sequences of words\n",
    "worked much better than anyone expected.  And we must have heard\n",
    "at some point that it is the transformer architecture that did allow\n",
    "training on sequences of words.\n",
    "We will explore a simplified transformer,\n",
    "and see that both: this architecture build\n",
    "a mapping between elements in a sequence,\n",
    "and also provides an easy way to scale the size of these models.\n",
    "We will need a few packages\n",
    "\n",
    "- `pip install torch`\n",
    "- `pip install transformers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from transformers import AutoTokenizer, AutoConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT is the first widely adopted transformer model\n",
    "(after a few small experimental ones).\n",
    "We will replicate some of its architecture.\n",
    "\n",
    "The \"uncased\" part of the model name is the version of BERT\n",
    "where tokenisation of words ignores case, i.e. \"octopus\"\n",
    "and \"Octopus\" are the same token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30522, 768, 3072, 512, 12)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "(\n",
    "    config.vocab_size,\n",
    "    config.hidden_size,\n",
    "    config.intermediate_size,\n",
    "    config.max_position_embeddings,\n",
    "    config.num_attention_heads,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The config of the model tells us how it is parametrized.\n",
    "The most relevant configuration values are:\n",
    "\n",
    "- `vocab_size`: the number of words in the entire corpus the model has been trained on,\n",
    "  plus a few extra special tokens.\n",
    "- `hidden_size`: size of the embedding vector for each token.\n",
    "- `intermediate_size`: internal size of the feed forward (`Linear`) layers after\n",
    "  the attention blocks, we will ignore this for simplicity\n",
    "- `max_position_embeddings`: the context window the model was trained with,\n",
    "  also the maximu value of a positional token (we ignore position for now).\n",
    "- `num_attention_heads:` again for simplicity we will use a single attention head,\n",
    "  a full model will have several heads that are concatenated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(30522, 768)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "embedding_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an embedding layer just like BERT's one.\n",
    "With the difference that our layer has not been trained.\n",
    "\n",
    "Another thing we need for our example is some text to feed\n",
    "our simplified LM with.\n",
    "From the project Guttenberg we take the book Alice in Wonderland, \n",
    "it has more than enough text for a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ng this eBook.\n",
      "\n",
      "Title: Alice’s Adventures in Wonderland\n",
      "\n",
      "Author: Lewis Carroll\n",
      "\n",
      "Release Date: January, 1991 [eBook #11]\n",
      "[Most recently updated: October 12, 2020]\n",
      "\n",
      "Language: English\n",
      "\n",
      "\n",
      "Produced by: Arthur DiBianca and David Widger\n",
      "\n",
      "*** START OF THE PROJECT GUTENBERG EBOOK ALICE’S ADVENTURES IN WONDERLAND ***\n",
      "\n",
      "[Illustration]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Alice’s Adventures in Wonderland\n",
      "\n",
      "by Lewis Carroll\n",
      "\n",
      "THE MILLENNIUM FULCRUM EDITION 3.0\n",
      "\n",
      "Contents\n",
      "\n",
      " CHAPTER I.     Down the Rabbit-Hole\n",
      " CHAPTER II.    The Pool of Tears\n",
      " CHAPTER III.  \n"
     ]
    }
   ],
   "source": [
    "with open(\"./lewis-carol-alice.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(text[512:1024])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the `vocab_size` enumerates words we need to know the numbers used\n",
    "by BERT itself.  We use the tokenizer from BERT itself hence.\n",
    "We need to know the value of `max_position_embeddings` as the tokenizer\n",
    "has a safety feature that would prevent us from feeding the model itself\n",
    "with more tokens than the contex window.\n",
    "\n",
    "For BERT `max_position_embeddings` is 512,\n",
    "We have not tokenized Alice in Wonderland so we do not know\n",
    "the number of words in the book.\n",
    "Instead we will use the comon statistic that the average word\n",
    "in english is 4 characters long.\n",
    "2048 characters will result in less than 512 words,\n",
    "since we also need to account for whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 450]),\n",
       " tensor([  101,  1996,  2622,  9535, 11029, 26885,  1997,  5650,  1521,  1055,\n",
       "          7357,  1999, 20365,  1010,  2011,  4572, 10767,  2023, 26885,  2003,\n",
       "          2005,  1996,  2224,  1997,  3087,  5973,  1999,  1996,  2142,  2163,\n",
       "          1998,  2087,  2060,  3033,  1997,  1996,  2088,  2012,  2053,  3465,\n",
       "          1998,  2007,  2471,  2053,  9259, 18971,  1012,  2017,  2089,  6100,\n",
       "          2009,  1010,  2507,  2009,  2185,  2030,  2128,  1011,  2224,  2009,\n",
       "          2104,  1996,  3408,  1997,  1996,  2622,  9535, 11029,  6105,  2443,\n",
       "          2007,  2023, 26885,  2030,  3784,  2012,  7479,  1012,  9535, 11029,\n",
       "          1012,  8917,  1012,  2065,  2017,  2024,  2025,  2284,  1999,  1996,\n",
       "          2142,  2163,  1010,  2017,  2097,  2031,  2000,  4638,  1996,  4277]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "inputs = tokenizer(text[:2048], return_tensors=\"pt\")\n",
    "inputs.input_ids.size(), inputs.input_ids[0][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenized words can now be fed through the embedding layer.\n",
    "\n",
    "The resulting matrix is of size words in context (max 512)\n",
    "times size of embeddings.\n",
    "This matrix is then the latent size inside LM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 450, 768])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = embedding_layer(inputs.input_ids)\n",
    "embeddings.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To return the latent size back to a tokenized representation (enum)\n",
    "we just need a matrix that looks like the transpose of the\n",
    "embedding layer.\n",
    "\n",
    "If this reminds anyone of autoencoders,\n",
    "it is not a coincidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 450, 30522])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_layer = nn.Linear(config.hidden_size, config.vocab_size)\n",
    "classification_layer\n",
    "\n",
    "next_tokens = classification_layer(embeddings)\n",
    "next_tokens.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have something very similar to the enum/one-hot of tokens\n",
    "as the output of our autoencoder language model.\n",
    "\n",
    "The difference is that we do not have a one-hot-encoded matrix.\n",
    "We got a matrix with real values everywhere.\n",
    "A `softmax` transformation will take care of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 450])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token_ids = F.softmax(next_tokens, dim=-1).max(dim=-1).indices\n",
    "next_token_ids.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`softmax` is just `max` but it is also a differentiable function.\n",
    "The fact that it is differentiable allows us to use some form\n",
    "of Gradient Descent on the output directly.\n",
    "\n",
    "And we now have a new set of tokens that can be transformed\n",
    "back into words by walking through the enum of the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rootedrga downloaded complimentedoom adequately neighboring aspect [unused786] insistent reactive blindness conquered glands severnhard国 method adequately mold rampantrga contacts neighboring capelice blindnessrga defenderladiers torchvius certificates neighboringrga guts they probability lucianoiers training philip probability23 himself criticism 1679 paler exercising acquitted glands atletico acquitted lacked heronmmanu contacts acquitted galicianrga gallagher neighboringrga downloaded complimentedoom vinnieometric training method adequately heron [unused984] they bonn criticism complimentedoom criticism fever criticism collapse 1679 overviewnies terminals blindnessrga defenderlad glands 1679食cars asshole detailingrga cell neighboringrga baskets interval 1679 overview terminals indicators gora method adequately criticism episodes [unused783] aspect [unused786] insistent reactive blindness conquered narrated [unused783]hard国 saving alma [unused783] lyle glandshorpe excellent adequatelyually conservatory looked excellent torch stole 仁 [unused783] pneumoniamare glands [unused363] looked possibility [unused783] [unused241] sharks severn [unused783] weights beaming 一 communitiesiers seriousnessetz pen anson exists exists exists enslaved neighboringrga downloaded complimentedoom adequately aspect [unused786] insistent reactive blindness conquered exists exists exists excellent filters looked aspect [unused786] insistent reactive blindness conquered severnhard国rga waterfront soul clock hussein historic stemmed criticismuba amenities tiles colonel criticism kimrga sayinganu [unused34] tiles call criticismrgaち neighboring ignorant tiles peaking criticism- joeanu gonzalesiers- buttonsyear tiles dysfunction criticismrga saying evenings blindness-ע sewing tiles winter criticism realityfus- rica seldom久 tiles evacuated criticism hasiers dome tileschaft criticism- disagreed universityanu forgiven tiles salle criticismrga sonya [unused786] insistent yucatan volunteers sharpenedanu penang tiles radiating criticismrga escapes conferred [unused786] insistent knockout tiles eater criticismrga botanic shit atkinson henson tilesyman criticism punta viewingrga papa ო description tiles dobson criticism aspect [unused786] insistent multiplayer tiles colonel criticism kimrga sayinganu [unused34] aspect included interval asshole resultanteti flowering neighboring bernardo severn viz [unused428]aisrga vatican glandsiers neighboring 1830s expensive asshole kingdom [unused783] ghana heron shortened promoters jewellery 37th computation arsrga [unused523] viz [unused428] included lori glands bandage acquitted jewellery probability violation heron portraying blindness acquitted glands [unused421]iers տ moldrga contacts neighboring- [unused523] glands rumours 512 aspect [unused421]ganj violation heron portraying description rumours legendary promoters included minimum blindness viz helsinki followers withstandears khalifaears promoters seniors glands rampantrga plastics horizon gunner viz eltoneti dominationiers programming angus glandsringrga pistols neighboring unaffected- stockanu alficles siberian sanctuaryrgauting neighboring bingham functioniers transylvaniarga stryker desirable glands ~ intends- [unused396] saying training welterweightitor crocodile treasure severn viz criticism 1997 includednies relegation rolf\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(next_token_ids[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is complete junk!  That's expected, we never trained this thing.\n",
    "\n",
    "But it also gives us one insight into how we can train it.\n",
    "We have words (tokens) as input and we have words (tokens) as output.\n",
    "So long as we use the same tokenizer on the input and output\n",
    "we do not need to worry about the internals!\n",
    "What the language model will do once trained is entirely dependent\n",
    "on how we setup the trainig regime.\n",
    "\n",
    "If we make the output to match against text further down in the\n",
    "Alice in Wonderland book, and backpropagate the differences,\n",
    "then we will have an LM that tells stories (most LLMs are trained that way).\n",
    "If we make the output match against a summary of the story\n",
    "then we will have a model great at summarising text.\n",
    "If we match against a translation we got a translator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can add the middle - first: Attention\n",
    "\n",
    "Yet LLMs oft do the tasks described above all at once.\n",
    "A simple autoencoder-style LM does not have enough parameters\n",
    "to process many tasks.  We need to add more parameters, more layers.\n",
    "\n",
    "Enters the transformer layer.  The clever way on how to scale LMs\n",
    "almost infinitelly.  The one thing that transforms LMs into LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 450, 450])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query, key, value = embeddings, embeddings, embeddings\n",
    "scores = torch.bmm(query, key.transpose(1, 2)) / math.sqrt(config.hidden_size)\n",
    "scores.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general the transformer layer builds a self-attention mapping\n",
    "within the latent values inside the LM.\n",
    "\n",
    "We process the values against each other.\n",
    "If we multiply the values against each other we get large\n",
    "values for where two values are large together in an input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 450, 450])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights = F.softmax(scores, dim=-1)\n",
    "attention_weights.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`softmax` once again smoothes these attention scores.\n",
    "This produces a mask of where only the high scores are present.\n",
    "\n",
    "Multiplying by that mask mltiplies together the original\n",
    "embedding values where the attention scores are high.\n",
    "Each token embedding is changed in value by multiplying\n",
    "with the values of token embeddings with which its attention is high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 450, 768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_outputs = torch.bmm(attention_weights, value)\n",
    "attention_outputs.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And transformer\n",
    "\n",
    "Finally we add the actual parameters, a simple NN layer.\n",
    "The entire trick is that this layer does not operate\n",
    "on the embedding but on the attention transformed embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_layer1 = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "\n",
    "def transformer1(embeddings: torch.Tensor):\n",
    "    query, key, value = embeddings, embeddings, embeddings\n",
    "    scores = torch.bmm(query, key.transpose(1, 2)) / math.sqrt(config.hidden_size)\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    attention_outputs = torch.bmm(attention_weights, value)\n",
    "    return linear_layer1(attention_outputs)\n",
    "\n",
    "linear_layer2 = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "\n",
    "def transformer2(embeddings: torch.Tensor):\n",
    "    query, key, value = embeddings, embeddings, embeddings\n",
    "    scores = torch.bmm(query, key.transpose(1, 2)) / math.sqrt(config.hidden_size)\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    attention_outputs = torch.bmm(attention_weights, value)\n",
    "    return linear_layer2(attention_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put the middle in\n",
    "\n",
    "Since each transformer layer has the input and output\n",
    "as the same laten size, we can stack as many of those as we want.\n",
    "Stacking many transformer layers is exactly how LLMs scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_causal_lm(text: str):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    embeddings = embedding_layer(inputs.input_ids)\n",
    "    x1 = transformer1(embeddings)\n",
    "    x2 = transformer2(x1)\n",
    "    token_ids = F.softmax(classification_layer(x2), dim=-1).max(dim=-1).indices\n",
    "    return tokenizer.decode(token_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note on multi-attention\n",
    "\n",
    "The use of the `softmax` in the attention layer above is quite oversimplified.\n",
    "A single set of attention scores and a singe `softmax` calculation will\n",
    "force one single main attention relationship for each input token.\n",
    "This is not how language works.  There may be more than a single main\n",
    "relationship between the words.\n",
    "\n",
    "In a modern implementation the attention scores are computed on subsets\n",
    "of the embeddings.  And each subset gets its own `softmax` calculation.\n",
    "The outputs are then summed togehter.  This is called multi-head attention,\n",
    "where each head is a calculation on one subset of the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[unused221]ulent divisional recognizingaldo backbone carolyn secretsvius libyan sydney holocaust intentions cutsmpt producers mckenzie pipeline backbone camille jawulent cid carolyn utility bosnian holocaustulent acquiredged laugh anger nightmares boot carolynulent cantoneselan hodges engineering laugh insults practiced hodges restrict nagoya [unused987] minute lighted trusts kellan cuts boris kellan illicit afrozzled ण cid kellanureulent activists carolynulent divisional recognizingaldo meteorological delight insults pipeline backbone afro guesslan pact [unused987] recognizingaldo [unused987] variables [unused987] hanson minute starring marry fraternity holocaustulent acquiredged cuts minute bro hedges borough warringtonulent corporate carolynulentkes purdue minute starring fraternity anaheim engraved pipeline backbone [unused987]dio gp secretsvius libyan sydney holocaust intentions businesses gp producers mckenzie blasting turing gp danzig cuts filippo 88 backbonennefan municipal 88 anger researched reporters gpige solvent cuts bey municipal incidents gp gallery hoovesmpt gp chemistry remarkably hornet inhabit laughjon televised invite cm wears wears wearssboro carolynulent divisional recognizingaldo backbone secretsvius libyan sydney holocaust intentions wears wears wears 88 610 municipal secretsvius libyan sydney holocaust intentionsmpt producers mckenzieulent њ home tone demos made francaise [unused987] bombers possesses leonbant [unused987] clownulent cursed ण wheels leon succeed [unused987]ulent couch carolyn deals leon anne [unused987] species travellers ण bowler laugh species chronology gunner leon trieste [unused987]ulent cursed partisans holocaust species stillsford leon louder [unused987] [unused80] gossip species dripping processing overcame leon ⱼ [unused987] frederick laugh insisting leon 278 [unused987] species tuckedums णuls leon sitcom [unused987]ulent kaylavius libyan distinguishes clancy brake ण english leon brien [unused987]ulent grover samsungvius libyan walkway leon partnering [unused987]ulent generation believing associate 20s leonし [unused987] zip federicoulent 95 analyzedbit leonbiology [unused987] secretsvius libyan racetrack leonbant [unused987] clownulent cursed ण wheels secrets [unused537] meek borough sebastien wear categorized carolyn [unused133]mpt eli wonderlandperationulent intake cuts laugh carolynuring [unused36] borough garland gp premiered afro argued mhzatory 22nd dowridgeulentnies eli wonderland [unused537] founding cuts paints kellanatory hodges dh afro courtesy holocaust kellan cuts cruisers laugh winds camilleulent cid carolyn speciesnies cutscko tonic secrets cruisers offers dh afro courtesybitcko clemson mhz [unused537] 10 holocaust eli shankar rt arrange bjp diplomats bjp mhz sock cuts jawulent toastast princeton eli hose wear iata laugh belong am cuts intelligentulent [unused521] carolyn示 species vic ण shaved ‡rcus preferringulent 武 carolyn hiram leisure laugh [unused107]ulent阿 harassment cuts parchment stirling species clearance cursed insultssselsuitguiedompt eli [unused987]tale [unused537] marry remarks mort'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_causal_lm(text[:2048])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to train this thing of course\n",
    "\n",
    "After that it will present reasonable text.  But how do we train it.\n",
    "There are several competing ways but the simplest option is to give half a context window\n",
    "as input and then compare a full context window to the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about agentic AI?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you remember the game we trained with RL?  It had the actions:\n",
    "\n",
    "0. Move up\n",
    "1. Move right\n",
    "2. Move down\n",
    "3. Move left\n",
    "\n",
    "We can make similar actions to something real, say medicine dosage.\n",
    "\n",
    "0. Increase dose\n",
    "1. Decrease dose\n",
    "2. Treatment successful, stop treatment\n",
    "3. Ask a human for help\n",
    "\n",
    "Where the input is a report from the patient and the output is connected to an API\n",
    "for a dosage system.  And then to an email address for action `3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ACTIONS = 4\n",
    "agent_layer = nn.Linear(config.hidden_size, N_ACTIONS)\n",
    "\n",
    "def simple_agentic_ai(text: str):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    embeddings = embedding_layer(inputs.input_ids)\n",
    "    x1 = transformer1(embeddings)\n",
    "    x2 = transformer2(x1)\n",
    "    # the last layer is the only thing that changes\n",
    "    action_ids = F.softmax(agent_layer(x2), dim=-1).max(dim=-1).indices\n",
    "    return action_ids.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we train all layers, then we add the changed last layer, and only then train only the new layer."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
