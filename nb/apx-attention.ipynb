{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAML Attention, Transformers and LLMs (and all the fancy buzzwords)\n",
    "\n",
    "Generative AI, from massive models to agents.\n",
    "These are models that have been trained on such plethora of data\n",
    "that they can solve distinct problems without re-training.\n",
    "One can argue that solveing language understanding\n",
    "the models have indirectly learned solutions to many\n",
    "problems by just leveraging the language structure in the training data.\n",
    "\n",
    "That is all fine and good, traning on sequences of words\n",
    "worked much better than anyone expected.  And we must have heard\n",
    "at some point that it is the transformer architecture that did allow\n",
    "training on sequences of words.\n",
    "We will explore a simplified transformer,\n",
    "and see that both: this architecture build\n",
    "a mapping between elements in a sequence,\n",
    "and also provides an easy way to scale the size of these models.\n",
    "We will need a few packages\n",
    "\n",
    "- `pip install torch`\n",
    "- `pip install transformers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from transformers import AutoConfig, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT is the first widely adopted transformer model\n",
    "(after a few small experimental ones).\n",
    "We will replicate some of its architecture.\n",
    "\n",
    "The \"uncased\" part of the model name is the version of BERT\n",
    "where tokenisation of words ignores case, i.e. \"octopus\"\n",
    "and \"Octopus\" are the same token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30522, 768, 3072, 512, 12)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "(\n",
    "    config.vocab_size,\n",
    "    config.hidden_size,\n",
    "    config.intermediate_size,\n",
    "    config.max_position_embeddings,\n",
    "    config.num_attention_heads,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The config of the model tells us how it is parametrized.\n",
    "The most relevant configuration values are:\n",
    "\n",
    "- `vocab_size`: the number of words in the entire corpus the model has been trained on,\n",
    "  plus a few extra special tokens.\n",
    "- `hidden_size`: size of the embedding vector for each token.\n",
    "- `intermediate_size`: internal size of the feed forward (`Linear`) layers after\n",
    "  the attention blocks, we will ignore this for simplicity\n",
    "- `max_position_embeddings`: the context window the model was trained with,\n",
    "  also the maximu value of a positional token (we ignore position for now).\n",
    "- `num_attention_heads:` again for simplicity we will use a single attention head,\n",
    "  a full model will have several heads that are concatenated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(30522, 768)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "embedding_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an embedding layer just like BERT's one.\n",
    "With the difference that our layer has not been trained.\n",
    "\n",
    "Another thing we need for our example is some text to feed\n",
    "our simplified LM with.\n",
    "From the project Guttenberg we take the book Alice in Wonderland, \n",
    "it has more than enough text for a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ng this eBook.\n",
      "\n",
      "Title: Alice’s Adventures in Wonderland\n",
      "\n",
      "Author: Lewis Carroll\n",
      "\n",
      "Release Date: January, 1991 [eBook #11]\n",
      "[Most recently updated: October 12, 2020]\n",
      "\n",
      "Language: English\n",
      "\n",
      "\n",
      "Produced by: Arthur DiBianca and David Widger\n",
      "\n",
      "*** START OF THE PROJECT GUTENBERG EBOOK ALICE’S ADVENTURES IN WONDERLAND ***\n",
      "\n",
      "[Illustration]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Alice’s Adventures in Wonderland\n",
      "\n",
      "by Lewis Carroll\n",
      "\n",
      "THE MILLENNIUM FULCRUM EDITION 3.0\n",
      "\n",
      "Contents\n",
      "\n",
      " CHAPTER I.     Down the Rabbit-Hole\n",
      " CHAPTER II.    The Pool of Tears\n",
      " CHAPTER III.  \n"
     ]
    }
   ],
   "source": [
    "with open(\"./lewis-carol-alice.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(text[512:1024])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the `vocab_size` enumerates words we need to know the numbers used\n",
    "by BERT itself.  We use the tokenizer from BERT itself hence.\n",
    "We need to know the value of `max_position_embeddings` as the tokenizer\n",
    "has a safety feature that would prevent us from feeding the model itself\n",
    "with more tokens than the contex window.\n",
    "\n",
    "For BERT `max_position_embeddings` is 512,\n",
    "We have not tokenized Alice in Wonderland so we do not know\n",
    "the number of words in the book.\n",
    "Instead we will use the comon statistic that the average word\n",
    "in english is 4 characters long.\n",
    "2048 characters will result in less than 512 words,\n",
    "since we also need to account for whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 450]),\n",
       " tensor([  101,  1996,  2622,  9535, 11029, 26885,  1997,  5650,  1521,  1055,\n",
       "          7357,  1999, 20365,  1010,  2011,  4572, 10767,  2023, 26885,  2003,\n",
       "          2005,  1996,  2224,  1997,  3087,  5973,  1999,  1996,  2142,  2163,\n",
       "          1998,  2087,  2060,  3033,  1997,  1996,  2088,  2012,  2053,  3465,\n",
       "          1998,  2007,  2471,  2053,  9259, 18971,  1012,  2017,  2089,  6100,\n",
       "          2009,  1010,  2507,  2009,  2185,  2030,  2128,  1011,  2224,  2009,\n",
       "          2104,  1996,  3408,  1997,  1996,  2622,  9535, 11029,  6105,  2443,\n",
       "          2007,  2023, 26885,  2030,  3784,  2012,  7479,  1012,  9535, 11029,\n",
       "          1012,  8917,  1012,  2065,  2017,  2024,  2025,  2284,  1999,  1996,\n",
       "          2142,  2163,  1010,  2017,  2097,  2031,  2000,  4638,  1996,  4277]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "inputs = tokenizer(text[:2048], return_tensors=\"pt\")\n",
    "inputs.input_ids.size(), inputs.input_ids[0][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenized words can now be fed through the embedding layer.\n",
    "\n",
    "The resulting matrix is of size words in context (max 512)\n",
    "times size of embeddings.\n",
    "This matrix is then the latent size inside LM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 450, 768])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = embedding_layer(inputs.input_ids)\n",
    "embeddings.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To return the latent size back to a tokenized representation (enum)\n",
    "we just need a matrix that looks like the transpose of the\n",
    "embedding layer.\n",
    "\n",
    "If this reminds anyone of autoencoders,\n",
    "it is not a coincidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Linear(in_features=768, out_features=30522, bias=True),\n",
       " torch.Size([1, 450, 30522]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_layer = nn.Linear(config.hidden_size, config.vocab_size)\n",
    "next_tokens = classification_layer(embeddings)\n",
    "\n",
    "classification_layer, next_tokens.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have something very similar to the enum/one-hot of tokens\n",
    "as the output of our autoencoder language model.\n",
    "\n",
    "The difference is that we do not have a one-hot-encoded matrix.\n",
    "We got a matrix with real values everywhere.\n",
    "A `softmax` transformation will take care of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 450])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token_ids = F.softmax(next_tokens, dim=-1).max(dim=-1).indices\n",
    "next_token_ids.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`softmax` is just `max` but it is also a differentiable function.\n",
    "The fact that it is differentiable allows us to use some form\n",
    "of Gradient Descent on the output directly.\n",
    "\n",
    "And we now have a new set of tokens that can be transformed\n",
    "back into words by walking through the enum of the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mesh panda rider soothing keyboardist dumping unrelated would intentions strategies protect iii¹ resumed circleworld [unused129]cated dumping rhythms recognized panda [unused737] unrelated cyprus [unused195] iii panda object ₊ sweep gunslingeryson prior unrelated panda equestrian cote overnightware sweep honored pharmaceuticals overnight impressive trim madam trough [unused666] westwood£ resumed ن£ necessary nationaltrom enigma [unused737]£⁷ pandazed unrelated panda rider soothing keyboardist 30 dale honoredcated dumping national oblast cote download madam soothing keyboardist madam genus madam ate trougheto tremendous tod iii panda object ₊ resumed trough bellevue quiz expected enhancement panda cushion unrelated pandagarhodon trougheto tod gunmen findcated dumping madam chopped bombed would intentions strategies protect iii¹ hire bombedworld [unused129]®ood bombed reduce resumedyse horribly dumping knockoutvanahini horribly gunslinger shout loyal bombed [unused764]ese resumed timelesshini hoarse bombed ⅔ devlin circle bombed shadow grammatical tobagotro sweepщ specializes 158bbleςςς aleksandr unrelated panda rider soothing keyboardist dumping would intentions strategies protect iii¹ςςς horriblytownhini would intentions strategies protect iii¹ circleworld [unused129] panda willingogo moved varietiesanu plug madam– isaac autobiographical radiated madamss panda elsa enigma tongues autobiographicaliable madam panda jared unrelated ： autobiographical count madam djs cycles enigma garland sweep djscycle blankly autobiographical 103 madam panda elsa papyrus iii djsilsdas autobiographicalwled madam meng ak djs fractured gutierrez bicycles autobiographicalnel madam filippo sweep pilgrim autobiographical plumbing madam djs scientific louis enigma internment autobiographical richer madam panda sediments intentions strategies prayed rice haydn enigma munich autobiographical growling madam panda wolves relying intentions strategies hume autobiographicalholder madam pandamic wound luigi entrances autobiographicaluel madam fat goalscorer panda flora ceremony 139 autobiographical airmen madam would intentions strategies hillside autobiographical radiated madamss panda elsa enigma tongues would observers philosophical expected walsh concepcion haired unrelated erika circle upgradesང dolls panda ottomans resumed sweep unrelated descent di expected ai bombed 212 national squirrel 271ych bets jacksonvillehoff panda neighborhoods upgradesང observers meets resumed floors£ych overnightナ national bologna iii£ resumedogist sweep zachary rhythms panda [unused737] unrelated djs neighborhoods resumed penned skeletal wouldogistپナ national bologna 139 pennedhul 271 observers reactive iii upgradesrco [unused176] blanca include ᅴ include 271hering resumed recognized panda gaius cylindrical天 upgrades angle concepcionwin sweep hiding dimly resumed edouard panda prominently unrelated crises djs‡ enigma caressed [unused710] curl goats panda marty unrelated spans pigeons sweep helsinki pandapy muscular resumed intended shit djs hartman elsa honoredfinder wolverhampton scrapped 1500 circle upgrades madam racial observers tremendous vane emerald\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(next_token_ids[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is complete junk!  That's expected, we never trained this thing.\n",
    "\n",
    "But it also gives us one insight into how we can train it.\n",
    "We have words (tokens) as input and we have words (tokens) as output.\n",
    "So long as we use the same tokenizer on the input and output\n",
    "we do not need to worry about the internals!\n",
    "What the language model will do once trained is entirely dependent\n",
    "on how we setup the trainig regime.\n",
    "\n",
    "If we make the output to match against text further down in the\n",
    "Alice in Wonderland book, and backpropagate the differences,\n",
    "then we will have an LM that tells stories (most LLMs are trained that way).\n",
    "If we make the output match against a summary of the story\n",
    "then we will have a model great at summarising text.\n",
    "If we match against a translation we got a translator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can add the middle - first: Attention\n",
    "\n",
    "Yet LLMs oft do the tasks described above all at once.\n",
    "A simple autoencoder-style LM does not have enough parameters\n",
    "to process many tasks.  We need to add more parameters, more layers.\n",
    "\n",
    "Enters the transformer layer.  The clever way on how to scale LMs\n",
    "almost infinitelly.  The one thing that transforms LMs into LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 450, 450])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query, key, value = embeddings, embeddings, embeddings\n",
    "scores = torch.bmm(query, key.transpose(1, 2)) / math.sqrt(config.hidden_size)\n",
    "scores.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general the transformer layer builds a self-attention mapping\n",
    "within the latent values inside the LM.\n",
    "\n",
    "We process the values against each other.\n",
    "If we multiply the values against each other we get large\n",
    "values for where two values are large together in an input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 450, 450])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights = F.softmax(scores, dim=-1)\n",
    "attention_weights.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`softmax` once again smoothes these attention scores.\n",
    "This produces a mask of where only the high scores are present.\n",
    "\n",
    "Multiplying by that mask mltiplies together the original\n",
    "embedding values where the attention scores are high.\n",
    "Each token embedding is changed in value by multiplying\n",
    "with the values of token embeddings with which its attention is high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 450, 768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_outputs = torch.bmm(attention_weights, value)\n",
    "attention_outputs.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And transformer\n",
    "\n",
    "Finally we add the actual parameters, a simple NN layer.\n",
    "The entire trick is that this layer does not operate\n",
    "on the embedding but on the attention transformed embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_layer1 = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "\n",
    "\n",
    "def transformer1(embeddings: torch.Tensor) -> torch.Tensor:\n",
    "    query, key, value = embeddings, embeddings, embeddings\n",
    "    scores = torch.bmm(query, key.transpose(1, 2)) / math.sqrt(\n",
    "        config.hidden_size\n",
    "    )\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    attention_outputs = torch.bmm(attention_weights, value)\n",
    "    return linear_layer1(attention_outputs)\n",
    "\n",
    "\n",
    "linear_layer2 = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "\n",
    "\n",
    "def transformer2(embeddings: torch.Tensor) -> torch.Tensor:\n",
    "    query, key, value = embeddings, embeddings, embeddings\n",
    "    scores = torch.bmm(query, key.transpose(1, 2)) / math.sqrt(\n",
    "        config.hidden_size\n",
    "    )\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    attention_outputs = torch.bmm(attention_weights, value)\n",
    "    return linear_layer2(attention_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put the middle in\n",
    "\n",
    "Since each transformer layer has the input and output\n",
    "as the same laten size, we can stack as many of those as we want.\n",
    "Stacking many transformer layers is exactly how LLMs scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_causal_lm(text: str) -> str:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    embeddings = embedding_layer(inputs.input_ids)\n",
    "    x1 = transformer1(embeddings)\n",
    "    x2 = transformer2(x1)\n",
    "    token_ids = F.softmax(classification_layer(x2), dim=-1).max(dim=-1).indices\n",
    "    return tokenizer.decode(token_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note on multi-attention\n",
    "\n",
    "The use of the `softmax` in the attention layer above is quite oversimplified.\n",
    "A single set of attention scores and a singe `softmax` calculation will\n",
    "force one single main attention relationship for each input token.\n",
    "This is not how language works.  There may be more than a single main\n",
    "relationship between the words.\n",
    "\n",
    "In a modern implementation the attention scores are computed on subsets\n",
    "of the embeddings.  And each subset gets its own `softmax` calculation.\n",
    "The outputs are then summed togehter.  This is called multi-head attention,\n",
    "where each head is a calculation on one subset of the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'witches debuting amateur into links revelation [unused786] shaggy 1928ignon nova alba afforded outdoorlde twists sicily 216 revelation commencing interference debutingnsis [unused786] ब ɹ alba debuting plastics ⟨ 720 goth preserve 1860 [unused786] debuting schedule zack bullyital 720ela parenting bullyisance年 docking むcarbon mesh tax outdooraki tax darrell classroom wipe teamsnsis tax breed debuting naturally [unused786] debuting amateur into links mammoth alumnusela 216 revelation classroom bulb zack warner docking into links dockingcuit docking mayer む lucy pacific daphne alba debuting plastics ⟨ outdoor むら folding [unused161] grazed debuting [unused521] [unused786] debuting wetland equations む lucy daphne champagne pacing 216 revelation docking barnsley lobby shaggy 1928ignon nova alba afforded lithium lobby twists sicily badminton suez lobby nikolai outdoor diego thousand revelation清غ worthless thousand gothʑ [unused117] lobby disclose awards outdoor over worthless pouring lobby །lakelde lobbydurdents ambientaring 720 textbooksٹ fixed b1 guardian guardian guardian fences [unused786] debuting amateur into links revelation shaggy 1928ignon nova alba afforded guardian guardian guardian thousand gavin worthless shaggy 1928ignon nova alba affordedlde twists sicily debuting norwood apostle mazdaкddled [unused260] docking threatens compounds reasonably arrested docking subsidies debuting fiscal teams imperative reasonably pod docking debuting nt [unused786] sitcom reasonably mediation docking mann handy teams threat 720 mann ن persia reasonablyobe docking debuting fiscal portal alba mann settlers engraver reasonablyoft docking 16th braun mann [unused241] [unused687] sas reasonably gallery docking autobiography 720 conferences reasonably censorship docking mann frankfurt egan teams thirst reasonably episcopal docking debuting bryant 1928ignonpee loving physics teams katz reasonably 112 docking debuting aristotle busch 1928ignon abel reasonablyvd docking debuting boutsu sucked perry reasonably cousins docking 165な debuting prayers seen unsigned reasonably allegedly docking shaggy 1928ignon form reasonably arrested docking subsidies debuting fiscal teams imperative shaggy maximus teams [unused161] chimneys tadeau [unused786] originalslde grounds northeasternnb debuting unhappy outdoor 720 [unused786] breakthrough vr [unused161] elector lobby nursery classroom while counsel showcase [unused353]metric47 debuting podium grounds northeastern maximus commentator outdoor sport tax showcase bully peggy classroom mayer alba tax outdoorれ 720 acute commencing debutingnsis [unused786] mann podium outdoor vincentkrishna shaggyれ cpu peggy classroom mayer unsigned vincent clay counsel maximus shriek alba grounds cpu palmertzer [unused205]ct [unused205] counsel hodges outdoor interference debutingschaftci enthusiast grounds mahogany tad clinton 720 throne rabbit outdoorona debutingcle [unused786] aziz mann erskine teams fatjean joked ア debuting necessitated [unused786] mascara finals 720 caress debuting syntax blast outdoor hopper shares mann collaborator fiscalelatre faux exemplified malleylde grounds docking subtropical maximus pacificれtens'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_causal_lm(text[:2048])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to train this thing of course\n",
    "\n",
    "After that it will present reasonable text.  But how do we train it.\n",
    "There are several competing ways but the simplest option is to give half a context window\n",
    "as input and then compare a full context window to the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about agentic AI?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you remember the game we trained with RL?  It had the actions:\n",
    "\n",
    "0. Move up\n",
    "1. Move right\n",
    "2. Move down\n",
    "3. Move left\n",
    "\n",
    "We can make similar actions to something real, say medicine dosage.\n",
    "\n",
    "0. Increase dose\n",
    "1. Decrease dose\n",
    "2. Treatment successful, stop treatment\n",
    "3. Ask a human for help\n",
    "\n",
    "Where the input is a report from the patient and the output is connected to an API\n",
    "for a dosage system.  And then to an email address for action `3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ACTIONS = 4\n",
    "agent_layer = nn.Linear(config.hidden_size, N_ACTIONS)\n",
    "\n",
    "\n",
    "def simple_agentic_ai(text: str) -> torch.Tensor:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    embeddings = embedding_layer(inputs.input_ids)\n",
    "    x1 = transformer1(embeddings)\n",
    "    x2 = transformer2(x1)\n",
    "    # the last layer is the only thing that changes\n",
    "    action_ids = F.softmax(agent_layer(x2), dim=-1).max(dim=-1).indices\n",
    "    return action_ids.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we train all layers, then we add the changed last layer, and only then train only the new layer."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
