{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAML Attention, Transformers and LLMs (and all the fancy buzzwords)\n",
    "\n",
    "Generative AI, from massive models to agents.\n",
    "These are models that have been trained on such plethora of data\n",
    "that they can solve distinct problems without re-training.\n",
    "One can argue that solveing language understanding\n",
    "the models have indirectly learned solutions to many\n",
    "problems by just leveraging the language structure in the training data.\n",
    "\n",
    "That is all fine and good, traning on sequences of words\n",
    "worked much better than anyone expected.  And we must have heard\n",
    "at some point that it is the transformer architecture that did allow\n",
    "training on sequences of words.\n",
    "We will explore a simplified transformer,\n",
    "and see that both: this architecture build\n",
    "a mapping between elements in a sequence,\n",
    "and also provides an easy way to scale the size of these models.\n",
    "We will need a few packages\n",
    "\n",
    "- `pip install torch`\n",
    "- `pip install transformers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from transformers import AutoTokenizer, AutoConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT is the first widely adopted transformer model\n",
    "(after a few small experimental ones).\n",
    "We will replicate some of its architecture.\n",
    "\n",
    "The \"uncased\" part of the model name is the version of BERT\n",
    "where tokenisation of words ignores case, i.e. \"octopus\"\n",
    "and \"Octopus\" are the same token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "config.vocab_size, config.hidden_size, config.intermediate_size, config.max_position_embeddings, config.num_attention_heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The config of the model tells us how it is parametrized.\n",
    "The most relevant configuration values are:\n",
    "\n",
    "- `vocab_size`: the number of words in the entire corpus the model has been trained on,\n",
    "  plus a few extra special tokens.\n",
    "- `hidden_size`: size of the embedding vector for each token.\n",
    "- `intermediate_size`: internal size of the feed forward (`Linear`) layers after\n",
    "  the attention blocks, we will ignore this for simplicity\n",
    "- `max_position_embeddings`: the context window the model was trained with,\n",
    "  also the maximu value of a positional token (we ignore position for now).\n",
    "- `num_attention_heads:` again for simplicity we will use a single attention head,\n",
    "  a full model will have several heads that are concatenated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "embedding_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an embedding layer just like BERT's one.\n",
    "With the difference that our layer has not been trained.\n",
    "\n",
    "Another thing we need for our example is some text to feed\n",
    "our simplified LM with.\n",
    "From the project Guttenberg we take the book Alice in Wonderland, \n",
    "it has more than enough text for a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./lewis-carol-alice.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(text[512:1024])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the `vocab_size` enumerates words we need to know the numbers used\n",
    "by BERT itself.  We use the tokenizer from BERT itself hence.\n",
    "We need to know the value of `max_position_embeddings` as the tokenizer\n",
    "has a safety feature that would prevent us from feeding the model itself\n",
    "with more tokens than the contex window.\n",
    "\n",
    "For BERT `max_position_embeddings` is 512,\n",
    "We have not tokenized Alice in Wonderland so we do not know\n",
    "the number of words in the book.\n",
    "Instead we will use the comon statistic that the average word\n",
    "in english is 4 characters long.\n",
    "2048 characters will result in less than 512 words,\n",
    "since we also need to account for whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "inputs = tokenizer(text[:2048], return_tensors=\"pt\")\n",
    "inputs.input_ids.size(), inputs.input_ids[0][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenized words can now be fed through the embedding layer.\n",
    "\n",
    "The resulting matrix is of size words in context (max 512)\n",
    "times size of embeddings.\n",
    "This matrix is then the latent size inside LM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embedding_layer(inputs.input_ids)\n",
    "embeddings.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To return the latent size back to tokenized (enum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_layer = nn.Linear(config.hidden_size, config.vocab_size)\n",
    "classification_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_tokens = classification_layer(embeddings)\n",
    "next_tokens.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_ids = F.softmax(next_tokens, dim=-1).max(dim=-1).indices\n",
    "next_token_ids.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(next_token_ids[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can add the middle - first: Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query, key, value = embeddings, embeddings, embeddings\n",
    "scores = torch.bmm(query, key.transpose(1, 2)) / math.sqrt(config.hidden_size)\n",
    "scores.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_weights = F.softmax(scores, dim=-1)\n",
    "attention_weights.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_outputs = torch.bmm(attention_weights, value)\n",
    "attention_outputs.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_layer1 = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "\n",
    "def transformer1(embeddings):\n",
    "    query, key, value = embeddings, embeddings, embeddings\n",
    "    scores = torch.bmm(query, key.transpose(1, 2)) / math.sqrt(config.hidden_size)\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    attention_outputs = torch.bmm(attention_weights, value)\n",
    "    return linear_layer1(attention_outputs)\n",
    "\n",
    "linear_layer2 = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "\n",
    "def transformer2(embeddings):\n",
    "    query, key, value = embeddings, embeddings, embeddings\n",
    "    scores = torch.bmm(query, key.transpose(1, 2)) / math.sqrt(config.hidden_size)\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    attention_outputs = torch.bmm(attention_weights, value)\n",
    "    return linear_layer2(attention_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put the middle in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_causal_lm(text: str):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    embeddings = embedding_layer(inputs.input_ids)\n",
    "    x1 = transformer1(embeddings)\n",
    "    x2 = transformer2(x1)\n",
    "    token_ids = F.softmax(classification_layer(x2), dim=-1).max(dim=-1).indices\n",
    "    return tokenizer.decode(token_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_causal_lm(text[:2048])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to train this thing of course\n",
    "\n",
    "After that it will present reasonable text.  But how do we train it.\n",
    "There are several competing ways but the simplest option is to give half a context window\n",
    "as input and then compare a full context window to the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about agentic AI?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you remember the game we trained with RL?  It had the actions:\n",
    "\n",
    "0. Move up\n",
    "1. Move right\n",
    "2. Move down\n",
    "3. Move left\n",
    "\n",
    "We can make similar actions to something real, say medicine dosage.\n",
    "\n",
    "0. Increase dose\n",
    "1. Decrease dose\n",
    "2. Treatment successful, stop treatment\n",
    "3. Ask a human for help\n",
    "\n",
    "Where the input is a report from the patient and the output is connected to an API\n",
    "for a dosage system.  And then to an email address for action `3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ACTIONS = 4\n",
    "agent_layer = nn.Linear(config.hidden_size, N_ACTIONS)\n",
    "\n",
    "def simple_agentic_ai(text: str):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    embeddings = embedding_layer(inputs.input_ids)\n",
    "    x1 = transformer1(embeddings)\n",
    "    x2 = transformer2(x1)\n",
    "    # the last layer is the only thing that changes\n",
    "    action_ids = F.softmax(agent_layer(x2), dim=-1).max(dim=-1).indices\n",
    "    return action_ids.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we train all layers, then we add the changed last layer, and only then train only the new layer."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
