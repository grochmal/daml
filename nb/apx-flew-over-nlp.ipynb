{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5047ff15-90df-41e7-a319-b9841c64f569",
   "metadata": {},
   "source": [
    "# One Flew over NLP\n",
    "\n",
    "A quick look through current NLP.\n",
    "We will look a Large Language Models (LLM), so popular in the recent years.\n",
    "And at a more practical library for NLP on a smaller scale - SpaCy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3d1245-1765-449b-9dd8-2db10d149c97",
   "metadata": {},
   "source": [
    "## LLMs\n",
    "\n",
    "GPT, PalM, Galactica, you name it.\n",
    "These popular models require hundreds of years in compute time to train.\n",
    "They learn language patterns which they can later replicate.\n",
    "The complete models contain hundreds of millions of neurons,\n",
    "and are often too expensive to just run, not to mention train on small problems.\n",
    "\n",
    "The [huggingface][hug] hub is one place where one can download\n",
    "reasonably sized pre-trained versions of popular models.\n",
    "These are often cut-to-size versions of the big trained models\n",
    "but with little loss of accuracy.\n",
    "\n",
    "[PyTorch][torch] integrates hunggingface hub as one of its model providers.\n",
    "Please note that this is often experimental code so one needs to fiddle with\n",
    "dependencies themselves.\n",
    "For example, for the following examples we currently need to install:\n",
    "\n",
    "    pip install torch\n",
    "    pip install transformers\n",
    "\n",
    "[hug]: https://huggingface.co/models\n",
    "[torch]: https://pytorch.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39948517-a195-4b44-ac86-2c84b25877d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "from torch import nn\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ee1cbc-6b25-47a5-887d-209e735c43fe",
   "metadata": {},
   "source": [
    "Similar to `sklearn` the `pipeline` builds an easy to use interface into the models.\n",
    "\n",
    "The important argument is the `task`,\n",
    "one needs to know the model to use for a given task\n",
    "as most models will only be capable of a few of the tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f06a062-6470-4369-b2e3-a446175f9b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function pipeline in module transformers.pipelines:\n",
      "\n",
      "pipeline(task: Optional[str] = None, model: Union[str, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel'), NoneType] = None, config: Union[str, transformers.configuration_utils.PretrainedConfig, NoneType] = None, tokenizer: Union[str, transformers.tokenization_utils.PreTrainedTokenizer, ForwardRef('PreTrainedTokenizerFast'), NoneType] = None, feature_extractor: Union[str, ForwardRef('SequenceFeatureExtractor'), NoneType] = None, image_processor: Union[str, transformers.image_processing_utils.BaseImageProcessor, NoneType] = None, processor: Union[str, transformers.processing_utils.ProcessorMixin, NoneType] = None, framework: Optional[str] = None, revision: Optional[str] = None, use_fast: bool = True, token: Union[str, bool, NoneType] = None, device: Union[int, str, ForwardRef('torch.device'), NoneType] = None, device_map=None, torch_dtype=None, trust_remote_code: Optional[bool] = None, model_kwargs: Optional[Dict[str, Any]] = None, pipeline_class: Optional[Any] = None, **kwargs) -> transformers.pipelines.base.Pipeline\n",
      "    Utility factory method to build a [`Pipeline`].\n",
      "    \n",
      "    A pipeline consists of:\n",
      "    \n",
      "        - One or more components for pre-processing model inputs, such as a [tokenizer](tokenizer),\n",
      "        [image_processor](image_processor), [feature_extractor](feature_extractor), or [processor](processors).\n",
      "        - A [model](model) that generates predictions from the inputs.\n",
      "        - Optional post-processing steps to refine the model's output, which can also be handled by processors.\n",
      "    \n",
      "    <Tip>\n",
      "    While there are such optional arguments as `tokenizer`, `feature_extractor`, `image_processor`, and `processor`,\n",
      "    they shouldn't be specified all at once. If these components are not provided, `pipeline` will try to load\n",
      "    required ones automatically. In case you want to provide these components explicitly, please refer to a\n",
      "    specific pipeline in order to get more details regarding what components are required.\n",
      "    </Tip>\n",
      "    \n",
      "    Args:\n",
      "        task (`str`):\n",
      "            The task defining which pipeline will be returned. Currently accepted tasks are:\n",
      "    \n",
      "            - `\"audio-classification\"`: will return a [`AudioClassificationPipeline`].\n",
      "            - `\"automatic-speech-recognition\"`: will return a [`AutomaticSpeechRecognitionPipeline`].\n",
      "            - `\"depth-estimation\"`: will return a [`DepthEstimationPipeline`].\n",
      "            - `\"document-question-answering\"`: will return a [`DocumentQuestionAnsweringPipeline`].\n",
      "            - `\"feature-extraction\"`: will return a [`FeatureExtractionPipeline`].\n",
      "            - `\"fill-mask\"`: will return a [`FillMaskPipeline`]:.\n",
      "            - `\"image-classification\"`: will return a [`ImageClassificationPipeline`].\n",
      "            - `\"image-feature-extraction\"`: will return an [`ImageFeatureExtractionPipeline`].\n",
      "            - `\"image-segmentation\"`: will return a [`ImageSegmentationPipeline`].\n",
      "            - `\"image-text-to-text\"`: will return a [`ImageTextToTextPipeline`].\n",
      "            - `\"image-to-image\"`: will return a [`ImageToImagePipeline`].\n",
      "            - `\"image-to-text\"`: will return a [`ImageToTextPipeline`].\n",
      "            - `\"mask-generation\"`: will return a [`MaskGenerationPipeline`].\n",
      "            - `\"object-detection\"`: will return a [`ObjectDetectionPipeline`].\n",
      "            - `\"question-answering\"`: will return a [`QuestionAnsweringPipeline`].\n",
      "            - `\"summarization\"`: will return a [`SummarizationPipeline`].\n",
      "            - `\"table-question-answering\"`: will return a [`TableQuestionAnsweringPipeline`].\n",
      "            - `\"text2text-generation\"`: will return a [`Text2TextGenerationPipeline`].\n",
      "            - `\"text-classification\"` (alias `\"sentiment-analysis\"` available): will return a\n",
      "              [`TextClassificationPipeline`].\n",
      "            - `\"text-generation\"`: will return a [`TextGenerationPipeline`]:.\n",
      "            - `\"text-to-audio\"` (alias `\"text-to-speech\"` available): will return a [`TextToAudioPipeline`]:.\n",
      "            - `\"token-classification\"` (alias `\"ner\"` available): will return a [`TokenClassificationPipeline`].\n",
      "            - `\"translation\"`: will return a [`TranslationPipeline`].\n",
      "            - `\"translation_xx_to_yy\"`: will return a [`TranslationPipeline`].\n",
      "            - `\"video-classification\"`: will return a [`VideoClassificationPipeline`].\n",
      "            - `\"visual-question-answering\"`: will return a [`VisualQuestionAnsweringPipeline`].\n",
      "            - `\"zero-shot-classification\"`: will return a [`ZeroShotClassificationPipeline`].\n",
      "            - `\"zero-shot-image-classification\"`: will return a [`ZeroShotImageClassificationPipeline`].\n",
      "            - `\"zero-shot-audio-classification\"`: will return a [`ZeroShotAudioClassificationPipeline`].\n",
      "            - `\"zero-shot-object-detection\"`: will return a [`ZeroShotObjectDetectionPipeline`].\n",
      "    \n",
      "        model (`str` or [`PreTrainedModel`] or [`TFPreTrainedModel`], *optional*):\n",
      "            The model that will be used by the pipeline to make predictions. This can be a model identifier or an\n",
      "            actual instance of a pretrained model inheriting from [`PreTrainedModel`] (for PyTorch) or\n",
      "            [`TFPreTrainedModel`] (for TensorFlow).\n",
      "    \n",
      "            If not provided, the default for the `task` will be loaded.\n",
      "        config (`str` or [`PretrainedConfig`], *optional*):\n",
      "            The configuration that will be used by the pipeline to instantiate the model. This can be a model\n",
      "            identifier or an actual pretrained model configuration inheriting from [`PretrainedConfig`].\n",
      "    \n",
      "            If not provided, the default configuration file for the requested model will be used. That means that if\n",
      "            `model` is given, its default configuration will be used. However, if `model` is not supplied, this\n",
      "            `task`'s default model's config is used instead.\n",
      "        tokenizer (`str` or [`PreTrainedTokenizer`], *optional*):\n",
      "            The tokenizer that will be used by the pipeline to encode data for the model. This can be a model\n",
      "            identifier or an actual pretrained tokenizer inheriting from [`PreTrainedTokenizer`].\n",
      "    \n",
      "            If not provided, the default tokenizer for the given `model` will be loaded (if it is a string). If `model`\n",
      "            is not specified or not a string, then the default tokenizer for `config` is loaded (if it is a string).\n",
      "            However, if `config` is also not given or not a string, then the default tokenizer for the given `task`\n",
      "            will be loaded.\n",
      "        feature_extractor (`str` or [`PreTrainedFeatureExtractor`], *optional*):\n",
      "            The feature extractor that will be used by the pipeline to encode data for the model. This can be a model\n",
      "            identifier or an actual pretrained feature extractor inheriting from [`PreTrainedFeatureExtractor`].\n",
      "    \n",
      "            Feature extractors are used for non-NLP models, such as Speech or Vision models as well as multi-modal\n",
      "            models. Multi-modal models will also require a tokenizer to be passed.\n",
      "    \n",
      "            If not provided, the default feature extractor for the given `model` will be loaded (if it is a string). If\n",
      "            `model` is not specified or not a string, then the default feature extractor for `config` is loaded (if it\n",
      "            is a string). However, if `config` is also not given or not a string, then the default feature extractor\n",
      "            for the given `task` will be loaded.\n",
      "        image_processor (`str` or [`BaseImageProcessor`], *optional*):\n",
      "            The image processor that will be used by the pipeline to preprocess images for the model. This can be a\n",
      "            model identifier or an actual image processor inheriting from [`BaseImageProcessor`].\n",
      "    \n",
      "            Image processors are used for Vision models and multi-modal models that require image inputs. Multi-modal\n",
      "            models will also require a tokenizer to be passed.\n",
      "    \n",
      "            If not provided, the default image processor for the given `model` will be loaded (if it is a string). If\n",
      "            `model` is not specified or not a string, then the default image processor for `config` is loaded (if it is\n",
      "            a string).\n",
      "        processor (`str` or [`ProcessorMixin`], *optional*):\n",
      "            The processor that will be used by the pipeline to preprocess data for the model. This can be a model\n",
      "            identifier or an actual processor inheriting from [`ProcessorMixin`].\n",
      "    \n",
      "            Processors are used for multi-modal models that require multi-modal inputs, for example, a model that\n",
      "            requires both text and image inputs.\n",
      "    \n",
      "            If not provided, the default processor for the given `model` will be loaded (if it is a string). If `model`\n",
      "            is not specified or not a string, then the default processor for `config` is loaded (if it is a string).\n",
      "        framework (`str`, *optional*):\n",
      "            The framework to use, either `\"pt\"` for PyTorch or `\"tf\"` for TensorFlow. The specified framework must be\n",
      "            installed.\n",
      "    \n",
      "            If no framework is specified, will default to the one currently installed. If no framework is specified and\n",
      "            both frameworks are installed, will default to the framework of the `model`, or to PyTorch if no model is\n",
      "            provided.\n",
      "        revision (`str`, *optional*, defaults to `\"main\"`):\n",
      "            When passing a task name or a string model identifier: The specific model version to use. It can be a\n",
      "            branch name, a tag name, or a commit id, since we use a git-based system for storing models and other\n",
      "            artifacts on huggingface.co, so `revision` can be any identifier allowed by git.\n",
      "        use_fast (`bool`, *optional*, defaults to `True`):\n",
      "            Whether or not to use a Fast tokenizer if possible (a [`PreTrainedTokenizerFast`]).\n",
      "        use_auth_token (`str` or *bool*, *optional*):\n",
      "            The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n",
      "            when running `huggingface-cli login` (stored in `~/.huggingface`).\n",
      "        device (`int` or `str` or `torch.device`):\n",
      "            Defines the device (*e.g.*, `\"cpu\"`, `\"cuda:1\"`, `\"mps\"`, or a GPU ordinal rank like `1`) on which this\n",
      "            pipeline will be allocated.\n",
      "        device_map (`str` or `Dict[str, Union[int, str, torch.device]`, *optional*):\n",
      "            Sent directly as `model_kwargs` (just a simpler shortcut). When `accelerate` library is present, set\n",
      "            `device_map=\"auto\"` to compute the most optimized `device_map` automatically (see\n",
      "            [here](https://huggingface.co/docs/accelerate/main/en/package_reference/big_modeling#accelerate.cpu_offload)\n",
      "            for more information).\n",
      "    \n",
      "            <Tip warning={true}>\n",
      "    \n",
      "            Do not use `device_map` AND `device` at the same time as they will conflict\n",
      "    \n",
      "            </Tip>\n",
      "    \n",
      "        torch_dtype (`str` or `torch.dtype`, *optional*):\n",
      "            Sent directly as `model_kwargs` (just a simpler shortcut) to use the available precision for this model\n",
      "            (`torch.float16`, `torch.bfloat16`, ... or `\"auto\"`).\n",
      "        trust_remote_code (`bool`, *optional*, defaults to `False`):\n",
      "            Whether or not to allow for custom code defined on the Hub in their own modeling, configuration,\n",
      "            tokenization or even pipeline files. This option should only be set to `True` for repositories you trust\n",
      "            and in which you have read the code, as it will execute code present on the Hub on your local machine.\n",
      "        model_kwargs (`Dict[str, Any]`, *optional*):\n",
      "            Additional dictionary of keyword arguments passed along to the model's `from_pretrained(...,\n",
      "            **model_kwargs)` function.\n",
      "        kwargs (`Dict[str, Any]`, *optional*):\n",
      "            Additional keyword arguments passed along to the specific pipeline init (see the documentation for the\n",
      "            corresponding pipeline class for possible values).\n",
      "    \n",
      "    Returns:\n",
      "        [`Pipeline`]: A suitable pipeline for the task.\n",
      "    \n",
      "    Examples:\n",
      "    \n",
      "    ```python\n",
      "    >>> from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer\n",
      "    \n",
      "    >>> # Sentiment analysis pipeline\n",
      "    >>> analyzer = pipeline(\"sentiment-analysis\")\n",
      "    \n",
      "    >>> # Question answering pipeline, specifying the checkpoint identifier\n",
      "    >>> oracle = pipeline(\n",
      "    ...     \"question-answering\", model=\"distilbert/distilbert-base-cased-distilled-squad\", tokenizer=\"google-bert/bert-base-cased\"\n",
      "    ... )\n",
      "    \n",
      "    >>> # Named entity recognition pipeline, passing in a specific model and tokenizer\n",
      "    >>> model = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
      "    >>> tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
      "    >>> recognizer = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
      "    ```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f622a4f1-9320-424d-aaa7-25f58a49bb63",
   "metadata": {},
   "source": [
    "One common task is mask-filling.\n",
    "Based on the context what word is most likely in the place of the mask.\n",
    "\n",
    "Pretty much any BERT based model is capable of this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77b42aa1-114c-45c7-b55c-969e9cd48493",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee2af35beaf54322b54314d7e4ff3468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "unmasker = pipeline(\"fill-mask\", model=\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9f0ca3-749f-4e8d-ac85-d4044399711d",
   "metadata": {},
   "source": [
    "And we can try a few masked sentences.\n",
    "\n",
    "The return from the `unmasker` is a list of ranked possibilities for the mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4833ab9d-43cf-4884-ada8-6b996e75e25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One flew over the [MASK] nest\n",
      "[{'score': 0.10374893993139267,\n",
      "  'sequence': 'one flew over the entire nest',\n",
      "  'token': 2972,\n",
      "  'token_str': 'entire'},\n",
      " {'score': 0.033855464309453964,\n",
      "  'sequence': 'one flew over the empty nest',\n",
      "  'token': 4064,\n",
      "  'token_str': 'empty'},\n",
      " {'score': 0.03381901606917381,\n",
      "  'sequence': 'one flew over the whole nest',\n",
      "  'token': 2878,\n",
      "  'token_str': 'whole'},\n",
      " {'score': 0.027772599831223488,\n",
      "  'sequence': 'one flew over the tree nest',\n",
      "  'token': 3392,\n",
      "  'token_str': 'tree'},\n",
      " {'score': 0.018109258264303207,\n",
      "  'sequence': 'one flew over the old nest',\n",
      "  'token': 2214,\n",
      "  'token_str': 'old'}]\n",
      "A humongous [MASK] entered the porcelain shop\n",
      "[{'score': 0.3986216187477112,\n",
      "  'sequence': 'a humongous figure entered the porcelain shop',\n",
      "  'token': 3275,\n",
      "  'token_str': 'figure'},\n",
      " {'score': 0.14282794296741486,\n",
      "  'sequence': 'a humongous voice entered the porcelain shop',\n",
      "  'token': 2376,\n",
      "  'token_str': 'voice'},\n",
      " {'score': 0.06942196190357208,\n",
      "  'sequence': 'a humongous man entered the porcelain shop',\n",
      "  'token': 2158,\n",
      "  'token_str': 'man'},\n",
      " {'score': 0.012670003809034824,\n",
      "  'sequence': 'a humongous woman entered the porcelain shop',\n",
      "  'token': 2450,\n",
      "  'token_str': 'woman'},\n",
      " {'score': 0.011280493810772896,\n",
      "  'sequence': 'a humongous visitor entered the porcelain shop',\n",
      "  'token': 10367,\n",
      "  'token_str': 'visitor'}]\n"
     ]
    }
   ],
   "source": [
    "cuckoo = \"One flew over the [MASK] nest\"\n",
    "print(cuckoo)\n",
    "pprint(unmasker(cuckoo))\n",
    "\n",
    "elephant = \"A humongous [MASK] entered the porcelain shop\"\n",
    "print(elephant)\n",
    "pprint(unmasker(elephant))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569b4f6a-326e-4f7c-8fa6-9b75a2ad298b",
   "metadata": {},
   "source": [
    "Masking is foten used to fing new terms for specific words.\n",
    "For example, to find synonyms in text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2715982-6597-40f4-9c71-a45e3bc5ab68",
   "metadata": {},
   "source": [
    "Another task we cam build a pipeline for is text generation.\n",
    "This is a more \"popular\" way to use LLMs for conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca5f85fa-785c-445c-a961-f630a2b6423f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cb8032ebdbd4afd822235de91f4f0bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f10fb65d4c54514be503d8756997d0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97c6fa08359c4720ad60d039df4a5a3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7dc7707c9994266bb4206e74725c8ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "792103ff31fc4d58add7a0cbef51f3fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff98e088457e46e2a629d41a07ba97d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ace34bc78f424aa1c318caf3fde9b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848dc1f3-d1c6-4a91-89c9-997c0914c422",
   "metadata": {},
   "source": [
    "A text generator produces a conversation-like text based on the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1690f8b0-a2cf-438f-b053-ce65fe331273",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Yesterday I went to town to talk with you about your new '\n",
      "                    'book, The Great American Detective. Do you think that you '\n",
      "                    \"can become a detective someday? I haven't read a book in \"\n",
      "                    'four years. Why? Why did this happen?\\n'\n",
      "                    '\\n'\n",
      "                    'James'}]\n"
     ]
    }
   ],
   "source": [
    "conversation = \"Yesterday I went to town to\"\n",
    "pprint(generator(conversation))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9907486a-eb6e-40d2-9445-9e08f45ada87",
   "metadata": {},
   "source": [
    "Note that LLM does not know whether the statements are true or not.\n",
    "The LLM only appears to be clever,\n",
    "it only knows what a human would like to read,\n",
    "not whether that fact is locgical or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45d52101-9c51-4983-a69b-28617f43fc27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Humans landed on Mars on Dec. 9, 1999. Credit: NASA '\n",
      "                    'Spitzer Space Telescope (USA)\\n'\n",
      "                    '\\n'\n",
      "                    'A new NASA Spitzer orbiter will bring the first '\n",
      "                    \"high-resolution images of the world's most distant star, \"\n",
      "                    'the far-'}]\n"
     ]
    }
   ],
   "source": [
    "conversation = \"Humans landed on Mars on\"\n",
    "pprint(generator(conversation))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e266ac47-311f-419f-8c89-6a25e9a6580e",
   "metadata": {},
   "source": [
    "### The pipeline\n",
    "\n",
    "We call the `pipeline` a pipeline in the cases above because it is composed of more than one model.\n",
    "Similar to the way as we did `PCA` and `k-means` in `sklearn`,\n",
    "the transformers pipeline glues a `tokenizer` and an embedding `model` together.\n",
    "\n",
    "The tokenizer is not too different from `tf-idf` in that it explodes words into hundreds of dimensions.\n",
    "The difference is that the dimensions tell more about the context of the words than in plain `tf-idf`.\n",
    "These high dimensional representations are then called word-embeddings.\n",
    "The models themselves then are huge attention neural nets which take input\n",
    "tokenised into these embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e07e09-ea14-4312-ba9a-4307716b4c60",
   "metadata": {},
   "source": [
    "## Down to Earth NLP\n",
    "\n",
    "A more practical set of NLP tools for common problems is provided by the SpaCy library.\n",
    "SpaCy uses a BERT based model to build embeddings and parts of speech for text.\n",
    "\n",
    "Instead of trying to be a input-output model SpaCy is a library of tools that uses\n",
    "a BERT model trained in several ways.\n",
    "One can then choose with ease how to manually craft an NLP system.\n",
    "\n",
    "Here the `en_core_web_sm` is the pre-trained BERT model that SpaCy should\n",
    "use to tokenize, parse and understand the given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6e2d07-3693-435b-a206-860f5499b28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy  # noqa\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\"merge_noun_chunks\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0516459-241c-4e59-b471-1190eb27ef24",
   "metadata": {},
   "source": [
    "One common taks for SpaCy is to identify action within a sentence.\n",
    "\n",
    "i.e. find the verb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a59e1d-94da-49e6-ae8f-db3ec5238efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = nlp(\"We Flew Over the Cuckoo's Nest\")\n",
    "print(list(sentence))\n",
    "[x.pos_ for x in sentence]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a00b5a7-6c61-42d9-9f49-a4d75ea8caee",
   "metadata": {},
   "source": [
    "Note that the model is context based.\n",
    "In a different context the word \"flew\" is not a verb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbbf2f3-5274-48db-b9aa-c77abc80968a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = nlp(\"One Flew Over the Cuckoo's Nest\")\n",
    "print(list(sentence))\n",
    "[x.pos_ for x in sentence]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23724e8c-9b76-401f-bd44-5fb939669b22",
   "metadata": {},
   "source": [
    "With a few tricks we can now find the action of the sentence.\n",
    "\n",
    "We will assume that the closest nouns to the verb re the actor and the actioned.\n",
    "There are better ways to do it but this way is easy enough to write."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e53e0f7-170b-483a-a697-508a903c0792",
   "metadata": {},
   "outputs": [],
   "source": [
    "NOUNS = [\"NOUN\", \"PRON\", \"PROPN\"]  # noun, pronoun, proper name\n",
    "sentence = nlp(\"We Flew Over the Cuckoo's Nest\")\n",
    "verbs = [x for x in sentence if x.pos_ == \"VERB\"]\n",
    "actions = []\n",
    "for v in verbs:\n",
    "    left = [le for le in v.lefts if le.pos_ in NOUNS]\n",
    "    right = [ri for ri in v.rights if ri.pos_ in NOUNS]\n",
    "    if left and right:\n",
    "        actions.append(f\"{left[-1]} - {v} -> {right[0]}\")\n",
    "actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe474ce-8075-4ae1-ac3f-e061c3860d09",
   "metadata": {},
   "source": [
    "Let's make a real example.\n",
    "\n",
    "Project Guttenberg is a collection of many books free of copyright.\n",
    "We take Alice's Adventures in Wonderland by Lewis Carol from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388e7b73-b63e-4ce7-af03-e3fb825cf418",
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_in_wonderland = open(\"lewis-carol-alice.txt\", \"r\").read()\n",
    "len(alice_in_wonderland)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7d6ede-7e1e-4ad5-87ae-666a0837cc2b",
   "metadata": {},
   "source": [
    "In plain text the entire book is about 160KBs.\n",
    "\n",
    "We can easily pass that through SpaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9449a038-6777-4b63-a96c-a7f73f13eaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "alice = nlp(alice_in_wonderland)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a948a0b-666c-4d77-9737-ac36f87f8522",
   "metadata": {},
   "source": [
    "Let's have a look at what we got.\n",
    "\n",
    "SpaCy has already separated the text into sentence for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc7b4b6-6ed5-491a-9e49-e67dadf9fe4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "list(islice(alice.sents, 3, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb9a62e-b6ab-4262-9145-2a1686307cae",
   "metadata": {},
   "source": [
    "With the exact same technique for finding actions\n",
    "we will search across the entire book.\n",
    "\n",
    "A helper class will aid us in the search later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a465a1a5-4402-426e-9eec-87e4b1d2cb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "NOUNS = [\"NOUN\", \"PRON\", \"PROPN\"]  # noun, pronoun, proper name\n",
    "\n",
    "\n",
    "class Action:\n",
    "    def __init__(self, left: str, verb: str, right: str) -> None:\n",
    "        self.left = left\n",
    "        self.verb = verb\n",
    "        self.right = right\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.left} - {self.verb} -> {self.right}\"\n",
    "\n",
    "    __str__ = __repr__\n",
    "\n",
    "\n",
    "def actions_in_sentence(sentence: str) -> list[Action]:\n",
    "    actions = []\n",
    "    verbs = [x for x in sentence if x.pos_ == \"VERB\"]\n",
    "    for v in verbs:\n",
    "        left = [ll for ll in v.lefts if ll.pos_ in NOUNS]\n",
    "        right = [rr for rr in v.rights if rr.pos_ in NOUNS]\n",
    "        if left and right:\n",
    "            actions.append(\n",
    "                Action(\n",
    "                    left[-1].text.replace(\"\\n\", \" \"),\n",
    "                    v.text,\n",
    "                    right[0].text.replace(\"\\n\", \" \"),\n",
    "                )\n",
    "            )\n",
    "    return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d8fe49-7880-460b-8875-f7d6b2cc3ef0",
   "metadata": {},
   "source": [
    "For the time beginning run it over 100 setneces only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300548e5-d9bd-4637-932d-669eae168163",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = []\n",
    "for sentence in islice(alice.sents, 100):\n",
    "    actions += actions_in_sentence(sentence)\n",
    "len(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a716e2-0d59-4790-ad0b-14a95f479eec",
   "metadata": {},
   "source": [
    "And that is a number of actions that we can visualise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd8e6bf-d60b-4832-99f2-425d013e7e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in actions:\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3013f3-7c06-4781-b959-62a6835c954a",
   "metadata": {},
   "source": [
    "Across the full book we got quite a number of actions.\n",
    "\n",
    "We can now try to figure out how active Alice is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40534165-eb70-4d2a-b846-1d2103450b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = []\n",
    "for sentence in alice.sents:\n",
    "    actions += actions_in_sentence(sentence)\n",
    "len(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdebc17-e9b0-4110-ac49-bfe8d25595fd",
   "metadata": {},
   "source": [
    "This is another simplification:\n",
    "we will not distinguish between the actor and the actioned upon.\n",
    "\n",
    "Statistically this should even out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a039d6c-297e-4ada-9c55-fb75d05b0d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(\n",
    "    [\n",
    "        a\n",
    "        for a in actions\n",
    "        if \"alice\" in a.left.lower() or \"alice\" in a.right.lower()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c4f293-6b05-4dcb-8869-7ff297bff63f",
   "metadata": {},
   "source": [
    "Alice is indeed quite active.\n",
    "\n",
    "We can compare this against other proeminent characters in the book:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96903a1-0fc2-444a-a6eb-26e5d8a974ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "cheshire_cat = len(\n",
    "    [a for a in actions if \"cat\" in a.left.lower() or \"cat\" in a.right.lower()]\n",
    ")\n",
    "hatter = len(\n",
    "    [\n",
    "        a\n",
    "        for a in actions\n",
    "        if \"hatter\" in a.left.lower() or \"hatter\" in a.right.lower()\n",
    "    ]\n",
    ")\n",
    "white_rabbit = len(\n",
    "    [\n",
    "        a\n",
    "        for a in actions\n",
    "        if \"hatter\" in a.left.lower() or \"hatter\" in a.right.lower()\n",
    "    ]\n",
    ")\n",
    "print(f\"Cheshire Cat: {cheshire_cat}\")\n",
    "print(f\"The Hatter: {hatter}\")\n",
    "print(f\"White Rabbit: {white_rabbit}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20626d1f-e7f4-4419-bc28-92caf8c75eb2",
   "metadata": {},
   "source": [
    "The above is quite a crude NLP system.\n",
    "A more complete system would require a way to recognise whether the nouns we find\n",
    "are indeed the characters we seek.\n",
    "And also recodnise pronouns refering to the characters.\n",
    "\n",
    "These tow sysmes are most often called Named Entity Recognition (NER)\n",
    "and Correference Resolution (Coref).\n",
    "Both systems are in active research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b924d41-53b4-4a04-bb89-f0823093641a",
   "metadata": {},
   "source": [
    "## Andrey Karpathy's miniGPT (simplified):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f2c3c4-9d49-457f-a9e8-d0982592f47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import namedtuple\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "Config = namedtuple(\"Config\", [\"n_embd\", \"n_head\", \"attn_pdrop\", \"resid_pdrop\"])\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A vanilla multi-head masked self-attention layer\n",
    "    with a projection at the end.\n",
    "    It is possible to use torch.nn.MultiheadAttention here\n",
    "    but I am including an explicit implementation here\n",
    "    to show that there is nothing too scary about transformers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: Config) -> None:\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn_k = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.c_attn_q = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.c_attn_v = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n",
    "        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, T, C = (\n",
    "            x.size()\n",
    "        )  # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch\n",
    "        # and move head forward to be the batch dim\n",
    "        k = self.c_attn_k(x)\n",
    "        q = self.c_attn_q(x)\n",
    "        v = self.c_attn_v(x)\n",
    "\n",
    "        # causal self-attention;\n",
    "        # Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        # skip connection\n",
    "        att = att + self.attn_dropout(att)\n",
    "        y = att @ v  # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2)\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(y)\n",
    "        return y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
