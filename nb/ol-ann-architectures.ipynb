{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.09 ANN Architectures\n",
    "\n",
    "Apart from multiple perceptrons interconnected with each other,\n",
    "several neural network architectures have been developed over the years.\n",
    "We did only see the autoencoder but many others exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Olympics](ol-olympics.svg)\n",
    "\n",
    "<div style=\"text-align:right;\"><sup>ol-olympics.svg</sup></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Architectures\n",
    "\n",
    "- Deep Neural Networks are simply networks with lots of layers,\n",
    "  but training layers deep through backpropagation turns to be hard.\n",
    "  A *vanishing gradient* problem it tackled by careful selection of activation functions.\n",
    "\n",
    "- Convolutional Neural Nets are networks with one or more convolutional layers,\n",
    "  these layers are not fully connected providing feature selection based on parts\n",
    "  of the input.\n",
    "\n",
    "- Recurrent Neural Networks have connections going backwards, i.e. the output\n",
    "  of a neuron feeds into the input of a neuron in the same or previous layer.\n",
    "  RNNs do not need to be deep networks but excel as such.\n",
    "\n",
    "- Long Short Term Memory (LSTM) are specifically constructed RNNs,\n",
    "  with very specific activation functions per layer.\n",
    "\n",
    "- Autoencoders are DNNs which can repeat the patterns they were presented with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Architectures\n",
    "\n",
    "- Hopfield Networks are early *autoencoders*, these could repeat a known\n",
    "  pattern when presented with a similar one.\n",
    "\n",
    "- Boltzman Machines are networks of probabilistic neurons where all neurons\n",
    "  are connected in all directions.\n",
    "  The input and output is done from the same neurons (visible neurons).\n",
    "\n",
    "- Restricted Boltzman Machines are BMs in which the hidden layer neurons\n",
    "  are not interconnected.\n",
    "  These are much easier to train than full BMs.\n",
    "\n",
    "- Deep Belief Networks are stacked RBMs on top of each other.\n",
    "  Each RBM can be trained separately, and we can stack several layers or RBMs.\n",
    "  These were the early DNNs.\n",
    "\n",
    "- Self-Organizing Maps are unsupervised networks for data visualization\n",
    "  and dimensionality reduction.\n",
    "  They use the concept that connections through which\n",
    "  data passes should be reinforced whilst all other connections should decay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN Libraries\n",
    "\n",
    "Today we still experience very fast evolution of NN techniques,\n",
    "and thanks to that also a very fast evolution of NN libraries.\n",
    "Some relevant names,\n",
    "all of which perform better than `sklearn` on NN building, follow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theano\n",
    "\n",
    "Was one of the first academic libraries that allowed transparent use of GPUs\n",
    "as if one was working with NumPy array.\n",
    "Theano started the trend of using a directed acyclic graphs DAG,\n",
    "to quickly compute derivatives.\n",
    "It was developed at the university of Montreal and is still used in many places.\n",
    "Currently new features ceased to be added to Theano\n",
    "because it was considered counter productive to develop it further\n",
    "as many libraries with commercial backing can do the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow and Keras\n",
    "\n",
    "The current top library for most NN computing in industry.\n",
    "It really is just a DAG processing engine on top of tensors.\n",
    "Where tensors are pretty much NumPy arrays with a couple of tricks\n",
    "to allow for quick computation of derivatives.\n",
    "Its main selling point is `tensorboard` a web UI to monitor the processing\n",
    "of the graph, and therefore monitor the network training.\n",
    "Tensorflow adopted the Keras library as its front end,\n",
    "allowing for a user friendly API.\n",
    "\n",
    "Issue with Tensorflow happen when one attempts to extend the library.\n",
    "The focus of Tensorflow is industry, and hence the quick development of known models.\n",
    "Research of completely new NN models can be tricky in Tensorflow.\n",
    "\n",
    "A demo of a simple tensorflow (limited to a handful of 2-dimensional problems)\n",
    "interface can be found at [tensorflow playground][tfpl]\n",
    "\n",
    "[tfpl]: http://playground.tensorflow.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch (pytorch)\n",
    "\n",
    "In the Python universe known as `pytorch` is a library written (mostly) in Lua.\n",
    "The library allows for almost complete freedom of matching and mixing NN parts.\n",
    "It is hence often used to research new methods in NNs.\n",
    "\n",
    "Just as its predecessors, Torch uses a DAG to compute derivatives.\n",
    "But it borrows from research in automatic graph building,\n",
    "and require less steps to preform the differentiation.\n",
    "Differentiation in torch is as easy as writing a function and asking\n",
    "for a derivative with a procedure.\n",
    "There is support for almost all array operations to be differentiated automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JAX, FLAX and Friends\n",
    "\n",
    "When one needs to go even further in the freedom of mixing and matching one\n",
    "goes all the way to the automatic differentiation libraries.\n",
    "The original library was `autograd` and one an still build reasonable NNs with it.\n",
    "\n",
    "The issue with `autograd` use is that it does not support GPU computations.\n",
    "For automatic differentiation and nothing else we got `JAX`.\n",
    "And on top of `JAX` we have currently a rather new development of `flax`,\n",
    "which will provide a NN interface to the automatic differentiation.\n",
    "\n",
    "Hence, for ultimate NN building freedom one would use `autograd`,\n",
    "or `JAX` if one has a GPU.\n",
    "But the API of these libraries can be quite a handful,\n",
    "and one need to understand NN internals very well in order to get\n",
    "anything out of them."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "daml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
