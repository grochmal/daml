{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAML 04 - Time Series\n",
    "\n",
    "Michal Grochmal <michal.grochmal@city.ac.uk>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with data we find ourselves defining dimensions over which we want to analyze it\n",
    "(aggregate it).  Dimension is a well known word in data warehousing and in analytic queries\n",
    "over such warehouses.  One such dimension that always appear for data analysis is the\n",
    "time dimension.  Windowing, changing granularity or aggregating over specific times in the\n",
    "time dimension is called time series analysis.\n",
    "\n",
    "Time series analysis requires us to be able to change the time dimension quickly, and tailor\n",
    "it to our current needs with little computation overhead.  `pandas` provides the tools for this\n",
    "through its time indexes: time stamps, time periods and time deltas.  Let's see how we build these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python has the `datetime` object built into the standard library but it is quite limited.\n",
    "There is also the [dateutil][] module, by Gustavo Niemeyer, which has a much better\n",
    "date parser; and the [pytz][], by Stuart Bishop, which allows to localize times and dates\n",
    "within and between timezones.  `pandas` makes use of all three of these modules to build its\n",
    "`Timestamp`, `Period` and `Timedelta` objects.\n",
    "\n",
    "[dateutil]: https://dateutil.readthedocs.io/en/stable/ \"dateutil documentation\"\n",
    "[pytz]: http://pythonhosted.org/pytz/ \"pytz documentation\"\n",
    "\n",
    "For example, the three dates below are parsed with the `dateutil` module behind the scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime('January, 2017'), pd.to_datetime('3rd of February 2016'), pd.to_datetime('6:31PM, Nov 11th, 2017')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date operations, e.g. the common cross-language `strftime`,\n",
    "work on `pandas` dates juts like on Python dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "date = pd.to_datetime('3rd of January 2018')\n",
    "date.strftime('%A')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like date operations, time deltas work too.\n",
    "And broardcasting works on deltas too (that `'D'` means \"day\", see next)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date + pd.to_timedelta(np.arange(12), 'D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexes on dates\n",
    "\n",
    "We can distinguish between three time definitions:\n",
    "the **timestamp**, e.g. at which time the plane did land;\n",
    "the **time period**, e.g. how many planes did land this Wednesday;\n",
    "and **time deltas** (or durations), e.g. how long ago did the last plane land.\n",
    "Each of these has a `pandas` object and index type:\n",
    "\n",
    "-   The `DatetimeIndex` is composed of `Timestamp` objects and is the most basic date index type.\n",
    "-   `PeriodIndex` uses `Period` objects which contain `start_time` and `end_time`\n",
    "    attributes to check whether a timestamp falls within the period.\n",
    "-   And the `TimedeltaIndex` is composed of `Timedelta` objects, which represent a duration of time.\n",
    "\n",
    "We can understand periods as aggregates of timestamps and are internally defined as a single\n",
    "timestamp (start of period) and a frequency (duration of the period).  All periods within a\n",
    "`PeriodIndex` must have the same frequency.  The frequency (or duration, or offset) in `pandas`\n",
    "can be defined in many ways, with letter codes.  The most important ones are:\n",
    "\n",
    "+ `D` - day\n",
    "+ `B` - day, business days only\n",
    "+ `W` - week\n",
    "+ `M` - month\n",
    "+ `A`/`Y` - year\n",
    "+ `H` - hour\n",
    "+ `T`/`min` - minute\n",
    "+ `S` - second\n",
    "\n",
    "And these can be combined in several ways\n",
    "(e.g. `BAS-APR` mean a year starting on 1st of April as the first business day).\n",
    "It is nearly impossible to remember all combinations,\n",
    "do have a link to the [offset documentation][offset] handy.\n",
    "Let's see how to create time based indexes:\n",
    "\n",
    "[offset]: http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases \"frequency codes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.to_datetime(['3rd of January 2016', '2016-Jul-6', '20170708'])\n",
    "dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those are `Timestamps`, we can convert that to `Periods`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates.to_period('D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And operations between dates result in `Timedelta`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates - dates[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of writing all dates we want in an index we can (and should) use the `_range` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.date_range('2017-11-29', '2017-12-03')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.date_range('2017-12-03', periods=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.date_range('2017-12-03', periods=10, freq='H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.period_range('2017-09', periods=8, freq='M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.timedelta_range(0, periods=10, freq='H')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fremont Bridge\n",
    "\n",
    "The [Fremont bridge][fremont] in Seattle is possibly on of the most studied bridges in the world,\n",
    "it sits between the Google office in Seattle and the Adobe offices.  It is preferred by cyclists\n",
    "due to the fact that it is a small bridge linking north Seattle and downtown (the other two bridges\n",
    "are motorway bridges).  Bicycle counters were installed on both sides of the bridge in 2012, and\n",
    "are collecting data to this day (as of time of writing).\n",
    "\n",
    "The data can be downloaded from  <http://data.seattle.gov/> ([direct link][dataset]) but we\n",
    "can use the `fremont-bridge.csv` file I have already downloaded.  The file has a `Date` column\n",
    "which we will parse as the index of our time series.  Then we will try to get an understanding\n",
    "of the data (e.g. using `describe`) and see if we can plot something interesting.\n",
    "\n",
    "A huge analysis of this data was performed by Jake VanderPlas in his blog.\n",
    "See the references at the end, his analysis goes much further than we go here.\n",
    "\n",
    "[fremont]: http://www.openstreetmap.org/#map=17/47.64813/-122.34965\n",
    "[dataset]: https://data.seattle.gov/Transportation/Fremont-Bridge-Hourly-Bicycle-Counts-by-Month-Octo/65db-xm6k)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../data/fremont-bridge.csv', index_col='Date', parse_dates=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks good but the column names are slightly long.\n",
    "Let's reduce column names and add a \"Total\" column for all bikes crossings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = ['West', 'East']\n",
    "data['Total'] = data['West'] + data['East']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better, now we can looks at some statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is aggregated by the hour, something that will be important to keep in mind.\n",
    "There is a handful of NaNs too but these are few and probably negligible.\n",
    "\n",
    "We should plot it to get a better understanding.\n",
    "We will need `matplotlib` configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-talk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = data.plot(alpha=0.6, figsize=(15, 6))\n",
    "plot.set_ylabel('bicycle count');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is too granular.\n",
    "We can see an annual trend in there but it is difficult to say much more.\n",
    "If we aggregate the bicycle count by week we should see more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly = data.resample('W').sum()\n",
    "plot = weekly.plot(style=[':', '--', '-'], figsize=(15, 6))\n",
    "plot.set_ylabel('bicycle count');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still very rigged, perhaps a rolling window over 30 days would be better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily = data.resample('D').sum()\n",
    "plot = daily.rolling(30, center=True).sum().plot(style=[':', '--', '-'], figsize=(15, 6))\n",
    "plot.set_ylabel('mean daily count');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's good enough.  The rolling window flattens the noise.\n",
    "The only thing we could improve towards a presentation of the graph is the number of points.\n",
    "Above every day is still a single point on the graph,\n",
    "we could smoothen it by making a window over 5 weeks instead of 30 days.\n",
    "For an even smoother curve we could do a rolling window over 10 weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly = data.resample('W').sum()\n",
    "plot = weekly.rolling(10, center=True).sum().plot(style=[':', '--', '-'], figsize=(15, 6))\n",
    "plot.set_ylabel('mean weekly count');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group By\n",
    "\n",
    "Similar to an RDBMS we can group by directly on the pandas data frame.\n",
    "On a time series *grouping by* can produce completely different time frames,\n",
    "known as dicing and slicing the frame.  In general we want to *group by* to\n",
    "first filter all rows into smaller groups and then apply the aggregation\n",
    "to each of these smaller groups.\n",
    "\n",
    "On dates and times this division is quite evident: each day is formed of hours,\n",
    "each hour of minutes and so on.  Let's first try to get a series of time stamps\n",
    "from our data and see how we can divide it into divisions such as weeks, days\n",
    "or hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = data.index.to_series()\n",
    "series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The date and time properties of the `Series` are on an attribute called `dt`.\n",
    "This attribute has [several properties][properties] that can be used to aggregate over.\n",
    "\n",
    "[properties]: http://pandas.pydata.org/pandas-docs/version/0.20/api.html#datetimelike-properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series.dt.dayofyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series.dt.dayofweek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's try out this *grouping by*.\n",
    "How many data points (hours) we have within each day of the week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series.groupby(series.dt.dayofweek).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many hours within each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series.groupby(series.dt.year).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hours within each of the days of the week but for 2012 only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series2012 = series['2012']\n",
    "series2012.groupby(series2012.dt.dayofweek).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, data collection started on a Wednesday so we do not have data for the first Monday and Tuesday.\n",
    "2012 ended in a Monday 31st of December, so we can see that there is an extra Monday in there.  2016\n",
    "was a leap year so we get 24 hours (and 24 data points more).\n",
    "\n",
    "But finding the missing data (remember the `data.info` above) from the `Series` is not going to be possible.\n",
    "This is because the series uses the index which is complete.  Yet, finding the missing data from\n",
    "the actual data frame is pretty easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[(data['West'].isnull()) | (data['East'].isnull())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's actually pretty random.  Device malfunction perhaps?  I'll argue that we can safely ignore\n",
    "these missing points and go back to our full datatset.\n",
    "\n",
    "Knowing about grouping by we can slice the data in more ways now.  One thing to note is that several\n",
    "of the properties of the `Series.dt` object are directly available from a `DatetimeIndex`.\n",
    "For example, grouping by time (we only have hours) we can get the mean hourly traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_time = data.groupby(data.index.time).mean()\n",
    "# where we want the ticks, do not let matplotlib choose\n",
    "hourly_ticks = 4 * 60 * 60 * np.arange(6)\n",
    "by_time.plot(xticks=hourly_ticks, style=[':', '--', '-'], figsize=(14, 7));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, yeah, those are people cycling to work and cycling back home.\n",
    "\n",
    "Grouping by the day of the week we get the amount of traffic across weekdays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_weekday = data.groupby(data.index.dayofweek).mean()\n",
    "# this is - again - a trick to get nice ticks\n",
    "weekdays = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "plot = by_weekday.plot(xticks=np.arange(len(weekdays)), style=[':', '--', '-'], figsize=(14, 9))\n",
    "plot.set_xticklabels(weekdays);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weekend is very different, as we'd expect.\n",
    "We can also check how the hourly distribution is on the weekend.\n",
    "We build a multi-index by *grouping by* over two elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekend = np.where(data.index.weekday < 5, 'Weekday', 'Weekend')\n",
    "by_time = data.groupby([weekend, data.index.time]).mean()\n",
    "by_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now one can select from the first index and plot each piece of data side to side.\n",
    "(Or one below the other, as here.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(15, 10))\n",
    "fig.subplots_adjust(hspace=0.3)\n",
    "hourly_ticks = 4 * 60 * 60 * np.arange(6)\n",
    "by_time.loc['Weekday'].plot(ax=ax[0], title='Weekdays',\n",
    "                            xticks=hourly_ticks, style=[':', '--', '-'])\n",
    "by_time.loc['Weekend'].plot(ax=ax[1], title='Weekends',\n",
    "                            xticks=hourly_ticks, style=[':', '--', '-']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References and Extras\n",
    "\n",
    "* [Python Data Science Handbook - Chapter 3: Pandas - Working with time series - Jake VanderPlas][1]\n",
    "* [Is Seattle Really Seeing an Uptick In Cycling? - Jake VanderPlas][2]\n",
    "\n",
    "[1]: https://jakevdp.github.io/PythonDataScienceHandbook/03.11-working-with-time-series.html\n",
    "[2]: https://jakevdp.github.io/blog/2014/06/10/is-seattle-really-seeing-an-uptick-in-cycling/"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
