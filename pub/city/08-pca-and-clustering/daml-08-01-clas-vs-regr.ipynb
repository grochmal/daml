{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 09.00 Classification vs Regression\n",
    "\n",
    "How is *classification* different from *regression*?\n",
    "Both techniques are a form of supervised learning,\n",
    "i.e. require training data to have some kind of labels or values which represent the desired output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that most classification methods can be used to perform regression\n",
    "and most regression methods can be used to perform classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from mpl_toolkits import mplot3d\n",
    "plt.style.use('seaborn-talk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "Imagine that we can construct a continuous function $f$ from a classification problem,\n",
    "and then say that values below some value of $f(\\vec{x}) = y_{decision}$ are from one class\n",
    "and above this value are from the other class.\n",
    "That is one common way of turning regression into binary classification (only two classes),\n",
    "yet points with values very close to $y_{decision}$ would be often misclassified.\n",
    "\n",
    "Instead, we can say that points with values close to $y_{decision}$ have a good probability\n",
    "of being from either of the classes (with a little higher probability for the class on which\n",
    "side of $y_{decision}$ the point is).\n",
    "Points away from $y_{decision}$ will then have a big probability of being of one of the classes\n",
    "and zero or almost zero probability of being from the other class.\n",
    "This is what logical regression performs.\n",
    "And several other classification techniques also perform this trick\n",
    "with a **decision function** below the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "People say that a stew needs to be cooked for a long time to be good.\n",
    "Let's see if we can classify stews based on their cooking time\n",
    "and how the people who ate them liked them:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Stew](sl-stew.svg)\n",
    "\n",
    "<div style=\"text-align:right;\"><sup>sl-stew.svg</sup></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'cooking time (h)': [0.2, 0.5, 0.6, 0.7, 1.0, 1.2, 1.3, 1.5, 1.9, 2.0, 2.5, 3.0, 3.7, 4.5],\n",
    "    'satisfied eater':  [  0,   0,   0,   1,   0,   1,   0,   0,   1,   1,   1,   1,   1,   1]})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that it is more likely that a better cooked steak leaves a satisfied eater.\n",
    "Yet where exactly is the boundary defined by this data is slightly more complicated.\n",
    "If the measures were equally distributed we could take the mean of the classes on a scaled space.\n",
    "But for that we would need to resample the cooking time.\n",
    "An easier way is to fit a model, and we will try  logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "model = LogisticRegression(C=1.5)\n",
    "data = df['cooking time (h)'].values[:, np.newaxis]\n",
    "y = df['satisfied eater']\n",
    "cross_val_score(model, data, y, cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we are fitting is a continuous *logit* function that attempts to find\n",
    "a good measure of the positive class (satisfied).\n",
    "\n",
    "$$\n",
    "\\min_{w,c} C \\sum_{i=1}^N log(exp(-y_i(X_i^T w + c) + 1) + \\frac{\\|w\\|_2^2}{2}\n",
    "$$\n",
    "\n",
    "This produces a value between 0 and 1 for every sample in the dataset\n",
    "and can be steered by the parameters inside the $w$ vector and the $C$ hyperparameter.\n",
    "$w$ contains a parameter for every feature.\n",
    "$C$ on the other hand is a *sensitivity* hyperparameter.\n",
    "A very high $C$ will make the logit function never leave a sample\n",
    "on the wrong side of the decision value, yet this may not converge.\n",
    "A very small $C$ will make the solution tend to the mean for all samples.\n",
    "As with most hyperparameters one should tune $C$ to the given problem.\n",
    "\n",
    "The logit function fit has no analytical solution,\n",
    "yet it is a convex problem with several constrains.\n",
    "We can solve it in several *numerical* ways.\n",
    "\n",
    "Let's use our trained model to check the probability of someone\n",
    "being satisfied by the stew cooked by the following times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(data, y)\n",
    "model.predict([[0.3], [2.6]]), model.predict_proba([[0.3], [2.6]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every result give us *two* probabilities,\n",
    "these are the probabilities to the negative and the positive classes, respectively.\n",
    "The negative class, in a binary classification is just $1 - y$ of the positive class.\n",
    "We can see it better if we plot both at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xfit = np.linspace(0, 5, 100)\n",
    "yfit = model.predict_proba(xfit[:, np.newaxis])\n",
    "fig, ax = plt.subplots(1, figsize=(15, 7))\n",
    "ax.plot(xfit, yfit[:, 0], color='crimson')\n",
    "ax.plot(xfit, yfit[:, 1], color='steelblue')\n",
    "ax.plot(data, y, 'o', color='black');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using *logit* for classification is actually much more common than for regression.\n",
    "Since logistic regression provides probabilities it is a good model to explain\n",
    "the classification performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression using KNN\n",
    "\n",
    "On the other hand we can also use a classification technique to perform a regression.\n",
    "When we count the neighbors in the $k$ nearest neighbors algorithm for classification\n",
    "we take the class of the majority of neighbors.\n",
    "But what if, instead of classes we have real values and we want a regressions?\n",
    "We could simply take the mean of all the neighbors.\n",
    "And this is exactly how a regressor based on KNN works.\n",
    "\n",
    "Onto a problem: The foundation depth of a crane depends on what lift it will need to carry,\n",
    "which in turn depends on the maximum weight and size (in the biggest dimension) of the load it may carry.\n",
    "Let's try to estimate that foundation depth.\n",
    "Here are some samples of foundation depth against their crane features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = np.linspace(0, 10, 10) + np.random.rand(10)\n",
    "size = np.linspace(0, 10, 10) + np.random.rand(10)\n",
    "w, s = np.meshgrid(weight, size)\n",
    "lift_foundation = w*3 + s*2 + 17 + 7*np.random.rand(10)\n",
    "fig = plt.figure(figsize=(16, 9))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(w, s, lift_foundation)\n",
    "data = np.dstack([w, s]).reshape(100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a set with two features,\n",
    "therefore we can visualize it easier with a 3 dimensional plot.\n",
    "We can see an almost-plane forming,\n",
    "we will estimate it based on data with a `KNeighborsRegressor`.\n",
    "Before we used the $R2$ measure to score the regressions,\n",
    "yet the $R2$ measure is particularly badly descriptive with KNN regressions.\n",
    "This is because the regression uses means to explain large regions\n",
    "and the variance of data within those regions is considered as \"unexplained\" by $R2$,\n",
    "instead we will use plain explained variance to score our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "model = KNeighborsRegressor(n_neighbors=3)\n",
    "grid = GridSearchCV(model, {'n_neighbors': [3, 4, 5, 6, 7]}, scoring='explained_variance', cv=5)\n",
    "grid.fit(data, lift_foundation.reshape(100))\n",
    "print(grid.best_score_)\n",
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a reasonable score therefore we can be confident in our model.\n",
    "One thing we should do is try to visualize how this model builds\n",
    "the values it predicts.\n",
    "We build a visualization by predicting lots of points with this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmesh, ymesh = np.meshgrid(np.linspace(0, 10, 100), np.linspace(0, 10, 100))\n",
    "xfit = np.dstack([xmesh, ymesh]).reshape(10000, 2)\n",
    "model = grid.best_estimator_\n",
    "yfit = model.fit(data, lift_foundation.reshape(100))\n",
    "yfit = model.predict(xfit)\n",
    "fig = plt.figure(figsize=(16, 9))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(xmesh, ymesh, yfit.reshape(100, 100));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like we can make areas where a certain class holds we can make areas\n",
    "where a certain continuous value holds.\n",
    "In simple words, the classification and regression problems are always related.\n",
    "The techniques for classification and/or regression try to\n",
    "either build a *decision function* across data points,\n",
    "or divide the search space into *specific areas* which hold a label or value."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
